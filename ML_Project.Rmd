---
title: "ML_Project"
author: "James Martherus"
date: "10/9/2016"
output: html_document
---

To begin, I loaded the data and packages we will need later:
```{r}
library(caret)
library(ggplot2)
library(reshape2)
library(randomForest)

train <- read.csv("/users/jamesmartherus/Documents/Coursera/Machine Learning/pml-training.csv")

test <- read.csv("/users/jamesmartherus/Documents/Coursera/Machine Learning/pml-testing.csv")
```

We then take a look at the data:
```{r}
train$classe <- as.factor(train$classe)
qplot(train$classe)
```

It appears that the variable we are predicting has five categories, with a bias towards category A.

Next we should do some preprocessing. First we get rid of all variables with missing values. If the model performs poorly, we can always add them back in and impute the missing values, but this may be unneccessary. Second, we get rid of some of the first variables in the dataset which are obviously useless.

```{r}
#Get rid of vars with missing values
train <- train[, colSums(is.na(train)) == 0]
test <- test[, colSums(is.na(test)) == 0]

#Get rid of some obviously useless variables
training <- train[, -c(1:7)]
testing <- test[, -c(1:7)]

#Get rid of variables with no variance
myNZVvars <- names(training) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
training <- training[!myNZVvars]

for (i in 1:length(testing) ) {
        for(j in 1:length(training)) {
        if( length( grep(names(training[i]), names(testing)[j]) ) ==1)  {
            class(testing[j]) <- class(training[i])
        }      
    }      
}


```


Then, we partition the training data into a true training set and a validation set. This allows us to gauge out of sample error without using our actual test data.
```{r}
inTrain <- createDataPartition(y=train$classe, p=.75, list=FALSE)
internal_train <- train[inTrain,]
internal_test <- train[-inTrain,]
```


Now we run a random forest model.
```{r}
set.seed(38576)
control <- trainControl(method = "cv", number = 5)
model1 <- train(classe ~ ., data = training, method = "rf", 
                   trControl = control)
print(model1, digits = 4)
```

Next we use the model to predict our validation data. Since we have good accuracy, we can now test the model on our actual testing data.
```{r}
# predict outcomes using validation set
predict_rf <- predict(model1, internal_test)
# Show prediction result
(conf_rf <- confusionMatrix(internal_test$classe, predict_rf))

(accuracy_rf <- conf_rf$overall[1])
```

It appears that the random forest model predicts whether the exercise was performed correctly quite well.
```{r}
(predict(model1, testing))
```

